[
  {
    "objectID": "code_standards.html#motivation",
    "href": "code_standards.html#motivation",
    "title": "Code standards",
    "section": "Motivation",
    "text": "Motivation\nWell written code is easy to read and understand. As a result, it is easy to amend, reuse and review. The actions below help to make code more readable and easier to edit and maintain."
  },
  {
    "objectID": "code_standards.html#use-open-source-languages-and-tools",
    "href": "code_standards.html#use-open-source-languages-and-tools",
    "title": "Code standards",
    "section": "1. Use open-source languages and tools",
    "text": "1. Use open-source languages and tools\nWe recommend using open-source tools for your coding work. The Central Digital and Data Office (CDDO) Service Manual recommends an “open source by default” approach for government code. Open source languages and tools make work more accessible for users, colleagues and stakeholders. They can access them without expensive licenses. Using open source also makes it easier to onboard people, get external review and collaborate across teams and departments.\nTools must be appropriate for the problem you are trying to solve. Python and R are usually best for analysis workflows. They are our approved coding languages for analysis and allow you to comply with this minimum standard. While you can use SQL in your code, a pipeline written entirely in SQL will not meet the standard.\nOpen-source languages also give you easy access to code packages created by others. There are large ecosystems of R and python packages designed for data science and statistics that can help you manipulate, analyse, and visualise data. Use standard, well tested packages in your code whenever you can, rather than attempting to reinvent the wheel. This saves time and increases resilience. Established packages with a large user base are usually extensively tested and have already had a lot of development work put into them."
  },
  {
    "objectID": "code_standards.html#minimise-repetition",
    "href": "code_standards.html#minimise-repetition",
    "title": "Code standards",
    "section": "2. Minimise repetition",
    "text": "2. Minimise repetition\nRepetitive code is hard to read. It also makes errors more likely. When the same logic repeats in multiple places you will have to change every single one when you update the pipeline. It is easy to miss something. Repetitive code is inefficient to write and document and makes it harder to review your work. In extreme cases, complex and highly repetitive code can be so difficult to understand and maintain that it has to be abandoned and rewritten completely. Although highly reptitive code can still be reproducible and quality assured, we highly advise minimising repetition in both ad hoc and re-usable code.\n\n2.1 Use control flow\nControl flow means changing the order that code runs in depending on the circumstances. This helps when you need to change and re-run your code often to adapt to different needs. For example, you can use if statements to make the same code behave differently in different situations. You can use loops to execute the same chunk of code a number of times or until a certain condition is met. [link to relevant course]\n\n\n2.2 Use functions\nFunctions are chunks of self-contained code that can be called from anywhere. Once you’ve written a function, you can execute it as many times as you need to by calling its name rather than copying the code it runs into your main pipeline lots of times. Functions can be written to receive different information each time, so you can apply them in different situations. For example, you can write a function to calculate a mean average that will work on any set of numbers. [link to relevant course]\n\n\n2.3 Make complex code modular\nCode is modular when it is broken down into multiple, self-contained pieces. This makes the code easier to read, test and change. [link to relevant courses] The best way to make code modular is to write most of your pipeline using functions or classes. Split long scripts into multiple modules, but make sure that you can still run the entire pipeline from one script or a single command. This is essential for code that is complex or intended for re-use, as larger code bases can quickly become difficult to manage."
  },
  {
    "objectID": "code_standards.html#make-your-code-readable-and-self-documenting",
    "href": "code_standards.html#make-your-code-readable-and-self-documenting",
    "title": "Code standards",
    "section": "3. Make your code readable and self-documenting",
    "text": "3. Make your code readable and self-documenting\nWell written code also needs less documentation. There are things you can do to make it much easier for an experienced programmer to pick up your code and use it without your help. Again, less readable code can still be reproducible, but writing more readable code will make your work more efficient and your code easier to review.\n\n3.1 Use a standard code style\nWe recommend using PEP8 for python and the Tidyerse or Google R Style Guide for R. These styles are designed to be readable and intuitive and experienced coders are familiar with them. If needed, you may want to adapt them a little for your local situation. While full compliance with well-established styles will maximise transparency, the main objective is to make sure everyone in your team agrees on and uses the same coding style.\n\n\n3.2 Everything that has a name in your code should have a name that is easy to understand\nThis includes things like variables, functions, modules, tests, scripts, and folders. Names should not be cryptic or obvious only to the people currently working on the code.\n\n\n3.3 It should be easy to understand which packages you use and where functions come from\nIn python, importing packages should be done at the top of the script. In R, avoid loading whole libraries using library(). [link to resources on this]. Instead, clearly reference the libraries you are using in the code using R namespaces – so use ggplot2::ggplot(), not just ggplot(). Make sure your documentation explains clearly which packages (in which versions) must be installed to run your code."
  },
  {
    "objectID": "documentation.html#motivation",
    "href": "documentation.html#motivation",
    "title": "Documentation",
    "section": "Motivation",
    "text": "Motivation\nEven well written code requires some documentation. Good documentation makes your code easier for someone else to understand and easier to work with. It also helps people who join the project or review the code. Documentation should always be up to date and stored alongside the code itself (for example as part of the code Gitlab or Github repository), so that people don’t have to search for it.\nDocumentation can also be excessive! Writing readable, automated code (see code standards) reduces the need for lots of code comments and lengthy desk instructions."
  },
  {
    "objectID": "documentation.html#document-everything-that-is-needed-to-write-and-run-the-code",
    "href": "documentation.html#document-everything-that-is-needed-to-write-and-run-the-code",
    "title": "Documentation",
    "section": "4. Document everything that is needed to write and run the code",
    "text": "4. Document everything that is needed to write and run the code\nYou should aim to document everything that a contributor would need to review, edit or run your code. These things should be documented alongside the code whenever possible, rather than in an external location. We have listed some essential forms of documentation below, although others may be appropriate, depending on the project.\n\n4.1 Create README files for every project\nAll pipelines should have a README file. README files are easy to create and maintain but are often neglected as part of pipeline development. These files should be the first thing that someone looking at your code reads. As a minimum, they should include: * Background - what the code is for and the process it executes. * Set up instructions, for example how to install the code, get all the supporting packages you need, prepare your input data, and configure the workflow. * How to run the code.\nDepending on the project, you might add other information to your README file. After reading the README, anyone should be able to install and run your code provided they have access to the data.\nIf you need detailed running instructions, you might create an extra markdown file for these and link it to your README file.\n\n\n4.2 Embed design documentation with code\nEvery project should have some design documentation. How much you need depends on the complexity of the work. At the very least it should cover what the pipeline does, what it uses as its inputs and what outputs it makes. This can be included in the README file. You can also use flow charts to show what the overall design looks like. It is also useful to link to other documentation about the methods used and why the workflow is set up as it is.\nDocumenting the design helps everyone understand what the code is supposed to achieve. It also helps you to assess whether the code is actually doing what it is supposed to do. Having a clear idea of the overall design and why it is set up this way also makes it clear whether the code could be improved.\n\n\n4.3 Document dependencies\nMost pipelines rely on external packages. Make it as easy as possible for others to replicate the correct environment when running the code. That means documenting the packages you use, for example using requirements files (python) or as part of package documentation (R) [add links].\nAt a minimum, you should have a list of dependencies. If you need specific versions of these for the code to work, make sure you let your users know which versions they can use.\n\n\n4.4 Record manual quality assurance and tests\nWhen tests and checks are not automated, record these alongside the code. Include manual checks and tests in desk instructions, for example in the README file or a separate document. This means other people running the pipeline will be able to replicate your quality assurance steps or verify the work you have done.\nFor manual tests and checks, maintain a clear record of what was tested, who by, when and what they found. The easiest way to do this is as part of a code review, which you can record using pull requests (GitHub) or merge requests (Gitlab) [add links]."
  },
  {
    "objectID": "documentation.html#document-what-the-code-is-doing",
    "href": "documentation.html#document-what-the-code-is-doing",
    "title": "Documentation",
    "section": "5. Document what the code is doing",
    "text": "5. Document what the code is doing\nYour scripts should contain enough documentation for other coders to be able to understand what each part of the code is intended to do. However, there is such a thing as too much documentation. Documentation inside the code should not repeat things that are documented elsewhere, e.g. the README file.\n\n5.1 Code comments should explain the “why” not the “how”\nUse code comments (other than function documentation) sparingly. Code comments explain why the code is written the way it is when it would not be obvious to an experienced programmer. They are not there to explain syntax to inexperienced programmers or to repeat in plain English the logic found in the code. Too many code comments make code harder to read and work with!\nThe one exception to this rule is code written for learning purposes. A lot of people assume code should be littered with comments, as the tutorial code they learned from was heavily commented. If colleagues struggle with understanding production code, you should support them to learn rather than turning pipeline code into tutorial code.\nExplanations of methodology or model assumptions do not belong in code comments. Rather, you should record them elsewhere and signpost them in your documentation. For example, you can create assumption or methodology documentation files and place them alongside the code in your repository.\n\n\n5.2 Document functions and classes\nIf your code uses functions and classes, you must document them appropriately. In python that means using docstrings. In R it means using roxygen2. These formats are especially useful when packaging code [add link], but we recommend using them for all your functions and classes because they give you a standard format to work with that makes it much easier to package code at a later date. Otherwise, you can use a block of code comments.\nGood function and class documentation reduces the need to clutter the code with comments and ensures you cover the most important information. Always document what a function or class takes in as its inputs, what it does, and what output it produces."
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Home",
    "section": "Aims",
    "text": "Aims\nThis document sets a minimum standard for Reproducible Analytical Pipelines (RAP) for ONS analysts. RAP refers to the use of tools and practices borrowed from software engineering to make our analysis more reproducible.\nAnalysis is reproducible if we can repeat our analysis process reliably. In practice, that means we can always produce the same outputs again given the same data. The “gold standard” for reproducibility is a pipeline that is robust and well documented enough that another analyst could reproduce your outputs with no support except your code, documentation, and data.\nThere is no single way to make analysis code reproducible. Rather, there are different ways of working that can help us get to that goal. We hope this document provides useful and coherent criteria for how RAP at ONS should look in practice.\nReproducibility is not an end in itself. Rather, it is necessary so our work has quality, value, and transparency. The minimum RAP standards are written with this in mind. The criteria below are there to make sure we get the most value out of reproducible analysis."
  },
  {
    "objectID": "index.html#how-should-the-standard-be-used",
    "href": "index.html#how-should-the-standard-be-used",
    "title": "Home",
    "section": "How should the standard be used?",
    "text": "How should the standard be used?\nThe standard sets a minimum set of expectations that code should comply with if it is used to produce analytical outputs. The analysis might be ad hoc analysis, to create official and national statistics or underpin other ONS analysis. While this is a minimum standard, there are many other techniques, tools and practices that can improve your code and make it even more efficient and reproducible. Once you comply with the minimum standard, we hope you will build on your work to make it even more resilient.\nWe do not expect all coding projects to meet the standard immediately. If your work does not comply with the minimum standard, you should work to put in place coherent and achievable plans for how you will achieve compliance.\nMost of the practices and activities here apply to both re-usable pipelines and ad hoc analysis. Criteria that only apply to re-usable pipelines are highlighted when they appear.\nLastly, this document is not intended to teach coding skills. For this, you can read our guidance on good coding practices or use our internal coding courses."
  },
  {
    "objectID": "index.html#accessibility-statement",
    "href": "index.html#accessibility-statement",
    "title": "Home",
    "section": "Accessibility statement",
    "text": "Accessibility statement\nThis accessibility statement applies to the Coding in Analysis and Research Survey report. Please note that this does not include third-party content that is referenced from this site.\nThe website is managed by the Methodology and Quality division of the Office for National Statistics. We would like this guidance to be accessible for as many people as possible. This means that you should be able to:\n\nchange colours, contrast levels and fonts\nzoom in up to 300% without the text spilling off the screen\nnavigate most of the website using just a keyboard\nnavigate most of the website using speech recognition software\nlisten to most of the website using a screen reader (including the most recent versions of JAWS, NVDA and VoiceOver)\n\n\nFeedback and reporting accessibility problems\nWe’re always looking to improve the accessibility of our guidance. If you find any problems not listed on this page or think that we’re not meeting accessibility requirements, please contact us by email at gsshelp@statistics.gov.uk. Please also get in touch if you are unable to access any part of this guidance, or require the content in a different format.\nWe will consider your request and aim to get back to you within 5 working days.\n\n\nEnforcement procedure\nThe Equality and Human Rights Commission (EHRC) is responsible for enforcing the Public Sector Bodies (Websites and Mobile Applications) (No. 2) Accessibility Regulations 2018 (the ‘accessibility regulations’). If you’re not happy with how we respond to your complaint, you should contact the Equality Advisory and Support Service (EASS)."
  },
  {
    "objectID": "index.html#contact-details",
    "href": "index.html#contact-details",
    "title": "Home",
    "section": "Contact details",
    "text": "Contact details\nWe would like to know what you think of the survey and the outputs. Please contact us if you have feedback on any of the following:\n\nOutputs/analyses you would like to see added to this research\nInformation you would like to see collected in the next iteration of CARS\nAny other comments\n\nIf you are interested in this survey or any of the questions asked, or want to give feedback, please contact ASAP@ONS.gov.uk."
  },
  {
    "objectID": "project_management.html#motivation",
    "href": "project_management.html#motivation",
    "title": "Project management",
    "section": "Motivation",
    "text": "Motivation\nGood project management is essential for making code sustainable and fit for purpose. This should not mean lots of additional meetings or box-ticking exercises! It is about making the right decisions at the right time to ensure coding work goes as smoothly as possible. You may already do some or all of the below."
  },
  {
    "objectID": "project_management.html#set-out-clear-aims-and-scope-for-each-project",
    "href": "project_management.html#set-out-clear-aims-and-scope-for-each-project",
    "title": "Project management",
    "section": "10. Set out clear aims and scope for each project",
    "text": "10. Set out clear aims and scope for each project\nA project should have clear aims before you begin coding. Without this, it will not be possible to determine what the proportionate level of quality assurance and documentation will be for the work you are doing. Deciding on your aims early on will keep your project focussed on the end goal.\nSimilarly, you should have a clear idea of what is and isn’t in scope for your coding project. It should be clear to everyone involved where your responsibilities begin and end. This will depend on many factors such as the number of business areas involved, the complexity of the pipeline and the IT systems involved. You should ensure other people involved in the analysis are aware of what will be in scope for the project to avoid scope creep, where you end up taking on more and more work that should not have been part of the project."
  },
  {
    "objectID": "project_management.html#set-out-your-quality-specifications",
    "href": "project_management.html#set-out-your-quality-specifications",
    "title": "Project management",
    "section": "11. Set out your quality specifications",
    "text": "11. Set out your quality specifications\nIdeally, all analysis code should be of the highest possible standard. Realistically, quality should be proportionate to complexity and risk. Highly complex and impactful work will require more testing, review, and documentation than a pipeline with lower impact and complexity. The practices included here should be proportionate and sufficient for most official statistics work, but you may need to do more for mission-critical pipelines.\nQuality specifications should not be determined by the skill level in your team. You should aim to upskill to be able to meet appropriate quality levels based on the complexity and risk of your analysis."
  },
  {
    "objectID": "project_management.html#plan-to-open-source-your-code-in-future-if-appropriate",
    "href": "project_management.html#plan-to-open-source-your-code-in-future-if-appropriate",
    "title": "Project management",
    "section": "12. Plan to open source your code in future (if appropriate)",
    "text": "12. Plan to open source your code in future (if appropriate)\nThe Government Service Manual states that code funded by the public should be made open source unless there is a good reason not to do so. Plan coding projects with open source in mind, even if open sourcing the code will not be possible for a while. If you can’t open source your code in its current form, consider whether there are things you can do to make it safe to open source. There is Analysis Function guidance on how to open source code effectively and how to decide whether to open source analytical code."
  },
  {
    "objectID": "project_management.html#set-out-roles-and-responsibilities",
    "href": "project_management.html#set-out-roles-and-responsibilities",
    "title": "Project management",
    "section": "13. Set out roles and responsibilities",
    "text": "13. Set out roles and responsibilities\nYou should know who is involved in your coding project and what their role is. This should be written down where everybody on the project can see it. People can have multiple roles and be responsible for multiple parts of the project. Recording roles and responsibilities early on and keeping them up to date means everybody knows who to turn to when important decisions need to be made or when something goes wrong. It also means you know when an important role is not covered and can manage this risk sensibly."
  },
  {
    "objectID": "project_management.html#have-a-succession-plan-in-place",
    "href": "project_management.html#have-a-succession-plan-in-place",
    "title": "Project management",
    "section": "14. Have a succession plan in place",
    "text": "14. Have a succession plan in place\nCode is not reproducible if only one person can use it! Single points of failure mean risks to resilience and reproducibility. Readable and well documented code is already much easier for someone else to pick up, but you should be able to cope with losing one of the coders on your team. Plan to have two people working on the code at all times. If this is impossible, work out a clear plan to bring in more coding resource swiftly, either through recruitment or by borrowing resource from another team. Escalate these risks if you cannot resolve them."
  },
  {
    "objectID": "ways_of_working.html#motivation",
    "href": "ways_of_working.html#motivation",
    "title": "Ways of working",
    "section": "Motivation",
    "text": "Motivation\nThese ways of working make RAP easier to implement and more productive. While coding standards are important, it’s just as important to manage your analysis code in the right way."
  },
  {
    "objectID": "ways_of_working.html#code-modules-should-run-end-to-end-without-manual-intervention",
    "href": "ways_of_working.html#code-modules-should-run-end-to-end-without-manual-intervention",
    "title": "Ways of working",
    "section": "6. Code modules should run end to end without manual intervention",
    "text": "6. Code modules should run end to end without manual intervention\nYou should not have to make manual changes to the code or modify parameters during execution. Pipeline scripts should run from end to end. This doesn’t mean automating everything. Rather, think about how best to break the workflow down into sensible steps. If you have manual QA checks to perform, you can do those after the pipeline has finished by building the checks into the run and saving run logs and QA outputs.\n\n6.1 Minimise manual steps\nAutomation isn’t just about efficiency. Automating simple manual steps reduces the risk of error. While those steps might be easy to carry out, each manual step risks human error. Automate analysis steps that computers can do more reliably than us, such as performing basic checks and saving outputs.\n\n\n6.2 Use configuration files\nAnalysts often write pipelines that need frequent adjustment, sometimes even while running the pipeline. For example, we often see scripts where variables need updating or code must be commented in and out (effectively adding or removing code chunks during execution). This is bad for reproducibility because it is very difficult to see what the code looked like for each pipeline run. It also makes the process hard to document, as the analyst needs to know which parts of it to change and when.\nYou can use configuration files to handle these changes outside of the code itself [add duck book link]. Configuration files store settings for your pipeline. For example, if you know you might need to manually change the input file name, it makes sense for it to be loaded in from the configuration file, so you don’t have to change every time it appears in your script. The file also acts as documentation, because it allows others to see which settings were used to run the code, without needing to read the entire code base. Other configuration settings might be things like dates, data and output folder locations, geographical areas, and model parameters.\nConfiguration files are especially useful for re-usable pipelines, like regular publications. However, they are also useful for one-off pieces work. Using a configuration file from the start makes code development easier if your code needs a lot of set up. For example, you might need to frequently change parameters when developing a model, even if the resulting model will be a single-use piece of work."
  },
  {
    "objectID": "ways_of_working.html#use-git-for-version-control",
    "href": "ways_of_working.html#use-git-for-version-control",
    "title": "Ways of working",
    "section": "7. Use git for version control",
    "text": "7. Use git for version control\nUsing git for version control means you will be able to see what previous versions of the code looked like. That means you can revert to previous iterations of your pipeline, which is very difficult to do manually. If used correctly, it will help you keep an audit trail of who made which contributions, when and why. This also improves efficiency by helping you to collaborate and track changes more effectively.\nAvoid archiving code in your repository. Using git means you can delete code that you don’t need as it’s preserved in the version history. Your repository should not contain archive folders or commented out code."
  },
  {
    "objectID": "ways_of_working.html#make-sure-your-code-meets-quality-standards",
    "href": "ways_of_working.html#make-sure-your-code-meets-quality-standards",
    "title": "Ways of working",
    "section": "8. Make sure your code meets quality standards",
    "text": "8. Make sure your code meets quality standards\nUltimately, the aim of RAP is to produce high quality statistical outputs. When thinking about the quality of your code you should consider how the ways you are working are helping you achieve this. This also means making sure code quality is proportionate to how complex and risky your pipeline is. Below are essential QA practices for analysis code, although some projects may also require additional forms of quality assurance.\n\n8.1 Make sure your analysis meets quality standards\nAnalysis carried out using code should meet the same quality standards as any other analysis. That means using the principles and practices set out in the AQuA book [link] and the ONS quality standard for analysis[link].\n\n\n8.2 Review all code as you go\nAim to review your entire code base at least once. Do this as you develop the code rather than at the end. If possible, code should be reviewed by someone who didn’t write it. Consider the quality of the code as well as whether it’s doing what it’s supposed to. [link to code review templates?]. In other words, use code reviews to make sure your code is readable, reproducible and well documented as well as whether it works.\n\n\n8.3 Test your code\nTest your code to make sure it does what you expected it to. Because code runs without errors and produces outputs that look plausible does not guarantee that it works as intended.\nThere are many ways to test code. Usually, testing is a lot easier when your code is modular or packaged. Make sure your tests are proportionate to risks and complexity. Ideally, tests should be formalised, whether they are manual or automated. Automating tests makes it much easier to document exactly how you tested your code and makes your tests easier to re-run. Ways to test code include: * Unit tests: automated testing of each function against expected outcomes * Integration tests: tests of how different functions work together * Parallel runs: running a new pipeline and comparing results with those of the existing pipeline * Running the code on different systems to ensure it produces a reliable outcome\nEach tests should serve a clear purpose. They are not a tick box exercise. Good tests will help you spot issues with the code before errors happen and speed up code production."
  },
  {
    "objectID": "ways_of_working.html#maintain-and-improve-your-code-continuously-applies-to-re-usable-code",
    "href": "ways_of_working.html#maintain-and-improve-your-code-continuously-applies-to-re-usable-code",
    "title": "Ways of working",
    "section": "9 Maintain and improve your code continuously (applies to re-usable code)",
    "text": "9 Maintain and improve your code continuously (applies to re-usable code)\nCode that is intended for re-use is never truly “finished”. You should expect to continue to maintain the code for as long as it needs to be used. Things like changes to your input data or the systems you use might require unexpected code changes.\n\n9.1 Make code improvements before errors occur\nCode is much harder to fix once an error has already occurred. Locating the source of the error and fixing the issue is much harder under pressure. Instead, you can focus on reducing risks in the code before they cause any errors. This migth mean making the code easier to change in future, doing more quality assurance and testing, or coding solutions for issues that are likely to arise in future, even if they haven’t happened before. Aim to improve your code even while it is functioning correctly to avoid future mistakes.\n\n\n9.2 Develop your code iteratively\nDon’t expect to develop everything to the highest standard right away. If your team are still learning to implement RAP you might aim for a basic set of code at first and improve on it over time. Be aware, however, that lower quality code will need more work in future compared with code that is well written, modular, documented and tested. Early code might also not fully meet user needs. You may want to initially prioritise the most important functionality and later add additional functionality/\n\n\n10 avoid reinventing the wheel the wheel (highly recommended)\nOpen source code is a great tool when it comes to building pipeline. There are usually solutions out there for common analysis problems that mean you don’t need to write everything from scratch. When tacking a new problem, it’s worth considering whether a package already exists that can help. We recommend using packages that are well established and actively maintained, which you can find out by visiting their github repositories or equivalents.\nThere are many resources online that can help you if such a package isn’t available. Forums like Stack Overflow carry solutions to common errors or bugs and users often offer code examples."
  }
]